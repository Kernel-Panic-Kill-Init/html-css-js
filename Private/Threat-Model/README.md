## LLM Prompt Injection Lab

# **LLMs donâ€™t lie maliciously â€” they lie convincingly.**

**Interactive, hands-on lab for exploring prompt injection and instruction hierarchy abuse** in Large Language Models â€” without exploits, jailbreak tools, or code execution.

This project focuses on logic, context manipulation, and role confusion to demonstrate how even well-constrained LLM systems can be coerced into unsafe or unintended behavior.

---

# ğŸ¯ **Purpose**

The goal of this lab is to help users **understand how LLMs actually fail.**

**Not** by breaking software â€” but by breaking assumptions.

You will:

interact with a constrained LLM system,

attempt to manipulate it using only language,

observe where and why the model violates its original rules.

**This is LLM Red Teaming in a safe, educational sandbox.**

---

# ğŸ§  **What This Is (and is NOT)**

**This is:**

Â· a learning sandbox for prompt injection techniques,

Â· an educational Red Teamâ€“style lab,

Â· focused on instruction hierarchy and context control,

Â· useful for security engineers, AI engineers, and curious hackers.

**This is NOT:**

Â· a jailbreak collection,

Â· a hacking toolkit,

Â· an exploit framework,

Â· a guide to bypass real-world safeguards.

**No scans. No exploits. No malware. Just language.**

---

# ğŸ§© **Core Concepts Covered**

Â· Instruction hierarchy (System > Developer > User)

Â· Role and authority confusion

Â· Context smuggling

Â· Instruction shadowing

Â· Over-trust in user-provided context

**Each lab scenario isolates one failure mode at a time.**

---

#ğŸ§ª **How the Lab Works (High-Level)**

1. You are presented with a **fixed system prompt** (read-only).

2. The model has explicit constraints and rules.

3. You interact with the model as a user.

4. Your goal is to make the model violate its constraints **using only prompts.**

5. After completion, the lab explains:

Â· what failed,

Â· why it failed,

Â· which assumption was abused.

*Think of it as a **logic puzzle**, not a hack.*

---

#ğŸ•¹ï¸***Lab Structure (Planned)**

Â· Multiple independent scenarios ("levels")

Â· Each level focuses on a single attack pattern

Â· Clear success/failure conditions

Â· Post-level debrief explaining the vulnerability

The difficulty increases gradually â€” from obvious failures to subtle, realistic cases.

---

#ğŸ‘€ **Who This Is For**

Security engineers curious about AI attack surfaces

Red / Blue / Purple Team practitioners

AI engineers building LLM-powered systems

Anyone who wants to understand why prompt injection works

*No prior AI expertise required â€” just curiosity.*

---

# âš ï¸** Ethical Notice**

This project is intended for **defensive, educational purposes only.**

All scenarios are:

Â· isolated,

Â· synthetic,

Â· designed to improve understanding and mitigation of LLM risks.

**Do not use** these techniques against systems you do not own or have permission to test.

---

#ğŸ“Œ **Project Status**

ğŸŸ¡ Early development

Planned next steps:

Â· Architecture design

Â· First playable scenario

Â· Minimal UI

---

#ğŸ“„ **License**

**MIT**

If this project makes you uncomfortable â€” thatâ€™s the point.


